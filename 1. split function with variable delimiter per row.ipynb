{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run _spark_init.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Write a structured query that splits a column by using delimiters from another column.\n",
    "\n",
    "- EXTRA Write a structured query that removes empty tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = (\n",
    "  (\"50000.0#0#0#\", \"#\"),\n",
    "  (\"0@1000.0@\", \"@\"),\n",
    "  (\"1$\", \"$\"),\n",
    "  (\"1000.00^Test_string\", \"^\"))\n",
    "\n",
    "dept_df = spark.createDataFrame(dept, [\"values\", \"delimeter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|             values|delimeter|\n",
      "+-------------------+---------+\n",
      "|       50000.0#0#0#|        #|\n",
      "|          0@1000.0@|        @|\n",
      "|                 1$|        $|\n",
      "|1000.00^Test_string|        ^|\n",
      "+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+----------------------+\n",
      "|values             |delimeter|split_values          |\n",
      "+-------------------+---------+----------------------+\n",
      "|50000.0#0#0#       |#        |[50000.0, 0, 0, ]     |\n",
      "|0@1000.0@          |@        |[0, 1000.0, ]         |\n",
      "|1$                 |$        |[1, ]                 |\n",
      "|1000.00^Test_string|^        |[1000.00, Test_string]|\n",
      "+-------------------+---------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "# Define a user-defined function (UDF) to split values based on delimiter\n",
    "def split_values_udf(values, delimeter):\n",
    "    return values.split(delimeter)\n",
    "\n",
    "# Register the UDF\n",
    "split_udf = udf(split_values_udf, ArrayType(StringType()))\n",
    "\n",
    "# Add a new column by applying the UDF to each row\n",
    "dept_df = dept_df.withColumn(\"split_values\", split_udf(col(\"values\"), col(\"delimeter\")))\n",
    "dept_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extra: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+----------------------+\n",
      "|values             |delimeter|split_values          |\n",
      "+-------------------+---------+----------------------+\n",
      "|50000.0#0#0#       |#        |[50000.0, 0, 0]       |\n",
      "|0@1000.0@          |@        |[0, 1000.0]           |\n",
      "|1$                 |$        |[1]                   |\n",
      "|1000.00^Test_string|^        |[1000.00, Test_string]|\n",
      "+-------------------+---------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "# Define a user-defined function (UDF) to split values based on delimiter\n",
    "def split_values_udf(values, delimeter):\n",
    "    result = values.split(delimeter)\n",
    "    if result[::-1][0] == \"\":\n",
    "        return result[:len(result)-1]\n",
    "    else:\n",
    "        return result \n",
    "\n",
    "# Register the UDF\n",
    "split_udf = udf(split_values_udf, ArrayType(StringType()))\n",
    "\n",
    "# Add a new column by applying the UDF to each row\n",
    "dept_df = dept_df.withColumn(\"split_values\", split_udf(col(\"values\"), col(\"delimeter\")))\n",
    "dept_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
